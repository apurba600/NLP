{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4904fade",
   "metadata": {},
   "source": [
    "### The aim of this project is to build a book recommendation system by using the concept that, books which are similar to each other will be referenced in the articles section(internal wiki links). It also consists of various tools and techniques that can be used to build other types of recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcc063f",
   "metadata": {},
   "source": [
    "\n",
    "# KEY LEARNINGS:\n",
    "\n",
    "- Importing JSON files\n",
    "- Dict use just like list comprehension\n",
    "- Using ITERTOOLS\n",
    "- Using collections, Counter and Ordereddict\n",
    "- using various loops within a loop\n",
    "- Using Generator function to alleviate the need to store all of the training data.\n",
    "\n",
    "\n",
    "#### Generator function learnings\n",
    "\n",
    "- Generator function helps to use the function effectively without saving everything to the memory. loop and use yeild\n",
    "- random class, .sample(population,k), .randrange(int val), .shuffle(list,dict), this library is very effective to randomize process\n",
    "- Generator needs some type of for, while loop, can just write the code and then say while True\n",
    "- when we use for loop with i, n enumerate(x), we can still call the i outside the for loop \n",
    "\n",
    "\n",
    "#### Words Embedding\n",
    "\n",
    "- Embedding is used to create a dense vector representing categories/words/sentences/item etc.\n",
    "- It is a good alternative to one hot encoding. The benefits of embedding is that we can lower the dimensions used and also find cosine similarities between the item.\n",
    "- Embedding is made easier by using the keras library. \n",
    "- When we use the library, It randomly initializes the embedding parameters, which is then trained to find the optimal parameters.\n",
    "- In this project, the supervised learning is done by creating a pair of positive/negative book and wikilinks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f5448e",
   "metadata": {},
   "source": [
    "# Import the libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbde19e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the libraries \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np # import array tools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, Dot, Reshape, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "from collections import Counter, OrderedDict\n",
    "from itertools import chain\n",
    "import json\n",
    "import random \n",
    "random.seed(101) # set random val, to generate same randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af0eabc",
   "metadata": {},
   "source": [
    "# Reading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d781e18",
   "metadata": {},
   "source": [
    "#### The file is already scraped from wikipedia and is saved in JSON format. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6ef297",
   "metadata": {},
   "source": [
    "#### Please download the raw file from the github and save in JSON format to call for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8ee0785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are37020 books.\n"
     ]
    }
   ],
   "source": [
    "books = list()\n",
    "\n",
    "with open('data_set/wiki_books_raw.ndjson', 'r') as fin:\n",
    "    \n",
    "    books = [json.loads(l) for l in fin]   # Append, # List comprehension\n",
    "\n",
    "books = [book for book in books if 'Wikipedia:' not in book[0]]\n",
    "print(f\"There are{len(books)} books.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95cc1f6",
   "metadata": {},
   "source": [
    "# Data cleaning and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "537bbf4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Freud: His Life and His Mind',\n",
       " {'1': '< !-- See Wikipedia:WikiProject_Books -- >',\n",
       "  'name': 'Freud: His Life and His Mind',\n",
       "  'image': 'File:Freud, His Life and His Mind (first edition).jpg',\n",
       "  'caption': 'Cover of the first edition',\n",
       "  'author': 'Helen Walker Puner',\n",
       "  'country': 'United States',\n",
       "  'language': 'English',\n",
       "  'subject': 'Sigmund Freud',\n",
       "  'publisher': 'Dell Publishing',\n",
       "  'pub_date': '1947',\n",
       "  'media_type': 'Print (Hardcover and Paperback)',\n",
       "  'pages': '288 (1959 edition)',\n",
       "  'isbn': '978-1560006114'},\n",
       " ['Sigmund Freud',\n",
       "  'Dell Publishing',\n",
       "  'Hardcover',\n",
       "  'Paperback',\n",
       "  'Sigmund Freud',\n",
       "  'Erich Fromm',\n",
       "  'Dell Publishing',\n",
       "  'Anna Freud',\n",
       "  'Ernest Jones',\n",
       "  'Carl Jung',\n",
       "  'Wilhelm Stekel',\n",
       "  'Fritz Wittels',\n",
       "  'Maurice English',\n",
       "  'The Nation',\n",
       "  'Frederick Crews',\n",
       "  'The New York Review of Books',\n",
       "  'Peter Gay',\n",
       "  'Freud: A Life for Our Time',\n",
       "  'The Life and Work of Sigmund Freud',\n",
       "  'Louis Breger',\n",
       "  'Wilhelm Fliess',\n",
       "  'Freud family',\n",
       "  'Cambridge University Press',\n",
       "  'John Wiley  &  Sons',\n",
       "  'Dell Publishing',\n",
       "  'Papermac',\n",
       "  'The Nation',\n",
       "  'The New York Review of Books',\n",
       "  'Category:1947 books',\n",
       "  'Category:Books about Sigmund Freud',\n",
       "  'Category:English-language books'],\n",
       " ['https://www.ebsco.com',\n",
       "  'http://www.nybooks.com/articles/2017/02/23/freud-whats-left/'],\n",
       " '2018-04-03T19:57:42Z',\n",
       " 3069]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480ab97b",
   "metadata": {},
   "source": [
    "#### It returns a list within a list. the 0 index within the list gives us the book title, 1 index postition gives book into and the 2 index position gives the internal wiki links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd293bbf",
   "metadata": {},
   "source": [
    "# Map books to integers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff4bd25",
   "metadata": {},
   "source": [
    "#### This step is necessay, as the embedding neural network requires the books to be in integer format. We will also reverse to get index to book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60c4d61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_index = {book[0]:i for i,book in enumerate(books)}  #Creates a dict of the book title with its index pos as value\n",
    "index_book = {i: book[0] for i,book in enumerate(books)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe94a494",
   "metadata": {},
   "source": [
    "### Explore the wikilinks, # of links, unique links and common links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff056e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1187773 total wikilinks and 311276 unique wiki links\n"
     ]
    }
   ],
   "source": [
    "wikilinks = list(chain(*[book[2] for book in books])) #We can use itertools to combine the list within a list \n",
    "print(f\"There are {len(wikilinks)} total wikilinks and {len(set(wikilinks))} unique wiki links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bae68399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50407 total wikilinks and 17032 unique wiki links\n"
     ]
    }
   ],
   "source": [
    "wikilinks_other_books = [link for link in wikilinks if link in book_index.keys()]\n",
    "print(f\"There are {len(wikilinks_other_books)} total wikilinks and {len(set(wikilinks_other_books))} unique wiki links\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8812fb03",
   "metadata": {},
   "source": [
    "### Most Linked Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8394035b",
   "metadata": {},
   "source": [
    "#### We want to count the links from each book once, We can take advantage of collections module to pass in a list and give out the sorted counted items in the list as dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cd25e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_val(listt):  ## Create a function that takes in a list and outputs a dict ordered\n",
    "    \n",
    "    counts = Counter(listt) # call Counter object and pass in the list, gives ordered vals.\n",
    "    \n",
    "    counts = sorted(counts.items(), key= lambda x: x[1], reverse = True)\n",
    "    counts = OrderedDict(counts)\n",
    "    \n",
    "    return counts # returns a dict in ordered format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bc8f598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hardcover', 7489),\n",
       " ('Paperback', 7311),\n",
       " ('Wikipedia:WikiProject Books', 6043),\n",
       " ('Wikipedia:WikiProject Novels', 6015),\n",
       " ('English language', 4185),\n",
       " ('United States', 3060),\n",
       " ('Science fiction', 3030),\n",
       " ('The New York Times', 2727),\n",
       " ('science fiction', 2502),\n",
       " ('novel', 1979)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_wikilinks = list(chain(*[list(set(book[2])) for book in books]))\n",
    "\n",
    "wikilink_counts = count_val(unique_wikilinks)\n",
    "list(wikilink_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c34914",
   "metadata": {},
   "source": [
    "#### lower case the links to get accurate number of links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d12050b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 297624 unique wikilinks.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('paperback', 8740),\n",
       " ('hardcover', 8648),\n",
       " ('wikipedia:wikiproject books', 6043),\n",
       " ('wikipedia:wikiproject novels', 6016),\n",
       " ('science fiction', 5665),\n",
       " ('english language', 4248),\n",
       " ('united states', 3063),\n",
       " ('novel', 2983),\n",
       " ('the new york times', 2742),\n",
       " ('fantasy', 2003)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikilinks = [link.lower() for link in unique_wikilinks]\n",
    "print(f\"There are {len(set(wikilinks))} unique wikilinks.\")\n",
    "\n",
    "wikilink_counts = count_val(wikilinks)\n",
    "list(wikilink_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f81489e",
   "metadata": {},
   "source": [
    "#### We also want to remove the most common links for the ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89acd58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = ['hardcover', 'paperback', 'hardback', 'e-book', 'wikipedia:wikiproject books', 'wikipedia:wikiproject novels']\n",
    "for t in to_remove:\n",
    "    wikilinks.remove(t)\n",
    "    _ = wikilink_counts.pop(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6104982a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41758\n"
     ]
    }
   ],
   "source": [
    "links = [t[0] for t in wikilink_counts.items() if t[1] >= 4]\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e03a21",
   "metadata": {},
   "source": [
    "#### Most linked to the books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ca6c54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The Encyclopedia of Science Fiction', 127),\n",
       " ('The Discontinuity Guide', 104),\n",
       " ('The Encyclopedia of Fantasy', 63),\n",
       " ('Dracula', 55),\n",
       " ('Encyclop√¶dia Britannica', 51)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find set of book wikilinks for each book\n",
    "unique_wikilinks_books = list(chain(*[list(set(link for link in book[2] if link in book_index.keys())) for book in books]))\n",
    "\n",
    "# Count the number of books linked to by other books\n",
    "wikilink_book_counts = count_val(unique_wikilinks_books)\n",
    "list(wikilink_book_counts.items())[0:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ad2318",
   "metadata": {},
   "source": [
    "#### PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de821ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Most Reapeated book')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABDAAAAKQCAYAAABpWIrCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA25klEQVR4nO3dd5htZXk3/u8tWECkyQk/xQJREkMsqGg0GkVN7C32DmrEN7G9UWNMzBuJxohRY41RggVr7ErUqEgEewEFAdGEWLHgsaBibODz+2M9w9lnmJkzB87Mfg5+Ptc116y96r3XXnuttb/7WWtXay0AAAAAI7vUvAsAAAAA2BIBBgAAADA8AQYAAAAwPAEGAAAAMDwBBgAAADA8AQYAAAAwPAEGAMAyquorVfWHywx7VVX9/Ros8/Cqeu22ni8AbO8EGACwHegfpH9RVXst6v/ZqmpVte/FnH+rqmuuMPzQqjq/qs6tqh9V1SlVdeeLs8y1VlUHV9VZ864DANg2BBgAsP34cpL7Lzyoqusk2Xkdl//x1touSXZP8pIk/1ZVu6/j8gGAX2MCDADYfrwmyUNmHh+S5NWzI1TVblX16qraWFVfraq/qapL9WHXrKoTquqHVfXdqnpj7/+hPvkpvYXFfVcqorX2q17L5ZPs3+dx2ap6TlV9rarOrqqXVtVOfdgeVfWuXtMPevdVZmo+vqqeWVWf6q073llVe84Mv0lVfayqzuktPw6eGfbQqjqjqn5cVV+qqkf2/pdP8h9Jrtyf07lVdeWqulRVPbmq/qeqvldVb1q0rAf39fa9qnrKKl6Tvarq2L78E6rq6jPz+v2q+nRf35+uqt+fGXblqjqmqr5fVWdW1SOWmnlVXbqq3lBVb62qy6yiHgC4xBJgAMD24xNJdq2q36mqHZLcL8nieyW8KMluSX4zyS0zBR4P7cOenuT9SfZIcpU+blprt+jDr9da26W19saViujLfmiSXyb5au99RJLfSnJgkmsm2SfJ3/Zhl0ryyiRXT3K1JD9N8uJFs31IkocluVKS85K8sC9rnyTvTvL3SfZM8sQkb62qDX267yS5c5Jde03Pq6obtNZ+kuQOSb7Zn9MurbVvJnlMkrv3dXPlJD9I8s99WQck+ZckD+7DrtjX00oemGm97pXk5CSv6/Pas9f9wj6ff0ry7qq6Yp/u35Kc1ZdzryT/UFW3np1xD4DekeTnSe7TWvvFFmoBgEu0aq3NuwYAYAuq6itJ/iTJTTK1fDghyRMyfUj/ZZL9knw9UzhwYGvt8326Rya5f2vt4Kp6dZKfJXlaa+2sRfNvSfZvrZ25zPIPTXJUknP78s9Lckhr7U1VVb3/dVtr/9PHv2mS17fW9ltiXgcm+WBrbY/++Pgkn2itPbk/PiBTGLBTpsDi2q21B89M/74+76OXmPc7+rxf0FtqvLa1Ntva44wkj26tHdcfXynJ1/qy/jrJAa21+/Vhl88UcNyxtfaBJZb1qiSXmxl/lyQ/TLJvkoOTPKa1duOZ8T+e5GVJjkvylSS7t9Z+3Ic9M8mVWmuHVtXhSW6QKYg6JcnjmhM2ANACAwC2M69J8oAkh2bR5SOZWgFcOptaRaR379O7n5Skknyqqk6vqodt5bI/0VrbPVMLjmOS/EHvvyHTvThO6pd5nJPkvb1/qmrnqnpZvzTjR0k+lGT33pJjwdcX1Xzp/nyunuTeC/Pt8755ppYaqao7VNUn+qUY5yS5Y59uOVdP8vaZeZ2R5Pwke2dqDXFBHb0Vx/e2sE5mxz83yff7fK6czV+Hhee1Tx/2/YXwYtGwBTdJct0kRwgvAGAiwACA7Uhr7auZbuZ5xyRvWzT4u5laY1x9pt/VknyjT/vt1tojWmtXTvLIJC+pFX55ZIUazk3yp0keXFXX78v9aZLfba3t3v926zf8TKaWIr+d5Pdaa7smWbhkpWZme9VFNf+yz/frSV4zM9/dW2uXb60dUVWXTfLWJM9JsncPV94zM9+lPvh/PckdFs3vcq21byT51mwdVbVzpss/VjI7/i6ZLnP5Zv+7+qJxF16LbybZs6qusMSwBe9P8swkx1XV3luoAQB+LQgwAGD78/Akt+4tBC7QWjs/yZuSPKOqrtBvKPn49PtkVNW9Z26e+YNMH/B/1R+fnem+GavSWvt+pktK/rbf1PNfM91/4jf6svapqtv10a+QKeA4p98b4qlLzPJBVXVADw2eluQt/fm8Nsldqup2VbVDVV2upp9HvUqSyyS5bJKNSc6rqjskue3MPM9OcsWq2m2m30v7+rl6r3NDVd2tD3tLkjtX1c37DTOfli2fK91xZvynZ2ql8vVMQcpvVdUDqmrHmm6MekCSd/XhH0vyzP58rpvpNd3sfiattX9M8vpMIcZKrUoA4NeCAAMAtjOttf9prZ24zODHJPlJki8l+UimD8Cv6MNulOSTVXVupktAHtda+1IfdniSo/ulFfdZZSnPz/QB/rpJ/jLJmUk+0S8T+UCmVhcL4+2UqUXFJzJdXrLYa5K8Ksm3k1wuyWP7c/16krtluj/FxkwtKP4iyaX6JRiPzRTa/CDTpTXHLMywtfaFJG9I8qX+vK6c5AV9nPdX1Y97Pb/Xxz89yaP6OvtWn+dm9wpZwuszBTLfT3LDJA/q8/peppuLPiHTZShPSnLn1tp3+3T3z3SvjG8meXuSpy51n43W2tMz3cjzA7O/lgIAv47cxBMAmKt+E8/XttaOmnctAMC4tMAAAAAAhifAAAAAAIbnEhIAAABgeFpgAAAAAMPbcd4FXBx77bVX23fffeddBgAAALCNnHTSSd9trW1Y3H+7DjD23XffnHjicr8iBwAAAGxvquqrS/V3CQkAAAAwPAEGAAAAMDwBBgAAADA8AQYAAAAwPAEGAAAAMDwBBgAAADA8AQYAAAAwPAEGAAAAMDwBBgAAADA8AQYAAAAwPAEGAAAAMDwBBgAAADA8AQYAAAAwPAEGAAAAMDwBBgAAADA8AQYAAAAwPAEGAAAAMDwBBgAAADA8AQYAAAAwPAEGAAAAMDwBBgAAADA8AQYAAAAwPAEGAAAAMDwBBgAAADA8AQYAAAAwPAEGAAAAMDwBBgAAADC8HeddwLzd8C9ePe8SGNhJz37IvEsAAAAgWmAAAAAA2wEBBgAAADA8AQYAAAAwPAEGAAAAMDwBBgAAADA8AQYAAAAwPAEGAAAAMDwBBgAAADA8AQYAAAAwPAEGAAAAMDwBBgAAADA8AQYAAAAwPAEGAAAAMDwBBgAAADA8AQYAAAAwPAEGAAAAMDwBBgAAADA8AQYAAAAwPAEGAAAAMDwBBgAAADA8AQYAAAAwPAEGAAAAMDwBBgAAADA8AQYAAAAwPAEGAAAAMDwBBgAAADA8AQYAAAAwvDULMKrqFVX1nao6babfs6vqC1X1uap6e1XtPjPsr6rqzKr6YlXdbq3qAgAAALY/a9kC41VJbr+o37FJrt1au26S/0ryV0lSVQckuV+S3+3TvKSqdljD2gAAAIDtyJoFGK21DyX5/qJ+72+tndcffiLJVXr33ZL8W2vt5621Lyc5M8mN16o2AAAAYPsyz3tgPCzJf/TufZJ8fWbYWb3fhVTVYVV1YlWduHHjxjUuEQAAABjBXAKMqnpKkvOSvG5rp22tHdlaO6i1dtCGDRu2fXEAAADAcHZc7wVW1aFJ7pzkNq211nt/I8lVZ0a7Su8HAAAAsL4tMKrq9kmelOSurbX/nRl0TJL7VdVlq2q/JPsn+dR61gYAAACMa81aYFTVG5IcnGSvqjoryVMz/erIZZMcW1VJ8onW2v9prZ1eVW9K8vlMl5Y8qrV2/lrVBgAAAGxf1izAaK3df4neL19h/GckecZa1QMAAABsv+b5KyQAAAAAqyLAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIa347wLALbsa0+7zrxLYGBX+9tT510CAACsOS0wAAAAgOEJMAAAAIDhCTAAAACA4QkwAAAAgOEJMAAAAIDhCTAAAACA4QkwAAAAgOEJMAAAAIDhCTAAAACA4QkwAAAAgOEJMAAAAIDhCTAAAACA4QkwAAAAgOEJMAAAAIDhrVmAUVWvqKrvVNVpM/32rKpjq+q/+/89ev+qqhdW1ZlV9bmqusFa1QUAAABsf9ayBcarktx+Ub8nJzmutbZ/kuP64yS5Q5L9+99hSf5lDesCAAAAtjNrFmC01j6U5PuLet8tydG9++gkd5/p/+o2+USS3avqSmtVGwAAALB9We97YOzdWvtW7/52kr179z5Jvj4z3lm934VU1WFVdWJVnbhx48a1qxQAAAAYxtxu4tlaa0naRZjuyNbaQa21gzZs2LAGlQEAAACjWe8A4+yFS0P6/+/0/t9IctWZ8a7S+wEAAACse4BxTJJDevchSd450/8h/ddIbpLkhzOXmgAAAAC/5nZcqxlX1RuSHJxkr6o6K8lTkxyR5E1V9fAkX01ynz76e5LcMcmZSf43yUPXqi4AAABg+7NmAUZr7f7LDLrNEuO2JI9aq1oAAACA7dvcbuIJAAAAsFoCDAAAAGB4AgwAAABgeAIMAAAAYHgCDAAAAGB4AgwAAABgeAIMAAAAYHgCDAAAAGB4AgwAAABgeAIMAAAAYHgCDAAAAGB4AgwAAABgeAIMAAAAYHgCDAAAAGB4AgwAAABgeAIMAAAAYHgCDAAAAGB4AgwAAABgeAIMAAAAYHgCDAAAAGB4AgwAAABgeAIMAAAAYHgCDAAAAGB4AgwAAABgeAIMAAAAYHgCDAAAAGB4AgwAAABgeAIMAAAAYHgCDAAAAGB4AgwAAABgeAIMAAAAYHgCDAAAAGB4AgwAAABgeAIMAAAAYHgCDAAAAGB4AgwAAABgeAIMAAAAYHgCDAAAAGB4AgwAAABgeAIMAAAAYHgCDAAAAGB4AgwAAABgeAIMAAAAYHgCDAAAAGB4AgwAAABgeAIMAAAAYHgCDAAAAGB4AgwAAABgeAIMAAAAYHgCDAAAAGB4AgwAAABgeAIMAAAAYHgCDAAAAGB4AgwAAABgeAIMAAAAYHgCDAAAAGB4AgwAAABgeAIMAAAAYHgCDAAAAGB4AgwAAABgeAIMAAAAYHgCDAAAAGB4AgwAAABgeAIMAAAAYHgCDAAAAGB4AgwAAABgeAIMAAAAYHgCDAAAAGB4AgwAAABgeAIMAAAAYHgCDAAAAGB4AgwAAABgeAIMAAAAYHgCDAAAAGB4AgwAAABgeAIMAAAAYHgCDAAAAGB4AgwAAABgeAIMAAAAYHgCDAAAAGB4cwkwqurPq+r0qjqtqt5QVZerqv2q6pNVdWZVvbGqLjOP2gAAAIDxrHuAUVX7JHlskoNaa9dOskOS+yV5VpLntdaumeQHSR6+3rUBAAAAY5rXJSQ7JtmpqnZMsnOSbyW5dZK39OFHJ7n7fEoDAAAARrPuAUZr7RtJnpPka5mCix8mOSnJOa218/poZyXZZ71rAwAAAMY0j0tI9khytyT7Jblykssnuf1WTH9YVZ1YVSdu3LhxjaoEAAAARjKPS0j+MMmXW2sbW2u/TPK2JDdLsnu/pCRJrpLkG0tN3Fo7srV2UGvtoA0bNqxPxQAAAMBczSPA+FqSm1TVzlVVSW6T5PNJPpjkXn2cQ5K8cw61AQAAAAOaxz0wPpnpZp2fSXJqr+HIJH+Z5PFVdWaSKyZ5+XrXBgAAAIxpxy2Psu211p6a5KmLen8pyY3nUA4AAAAwuHn9jCoAAADAqgkwAAAAgOEJMAAAAIDhCTAAAACA4QkwAAAAgOEJMAAAAIDhCTAAAACA4QkwAAAAgOEJMAAAAIDhCTAAAACA4QkwAAAAgOEJMAAAAIDhCTAAAACA4QkwAAAAgOEJMAAAAIDhCTAAAACA4QkwAAAAgOEJMAAAAIDhCTAAAACA4QkwAAAAgOEJMAAAAIDhCTAAAACA4QkwAAAAgOEJMAAAAIDhCTAAAACA4QkwAAAAgOEJMAAAAIDhCTAAAACA4QkwAAAAgOEJMAAAAIDhCTAAAACA4QkwAAAAgOEJMAAAAIDhCTAAAACA4QkwAAAAgOEJMAAAAIDhCTAAAACA4W0xwKiqHarqC+tRDAAAAMBSthhgtNbOT/LFqrraOtQDAAAAcCE7rnK8PZKcXlWfSvKThZ6ttbuuSVUAAAAAM1YbYPy/Na0CAAAAYAWrCjBaaydU1dWT7N9a+0BV7Zxkh7UtDQAAAGCyql8hqapHJHlLkpf1Xvskecca1QQAAACwmdX+jOqjktwsyY+SpLX230l+Y62KAgAAAJi12gDj5621Xyw8qKodk7S1KQkAAABgc6sNME6oqr9OslNV/VGSNyf597UrCwAAAGCT1QYYT06yMcmpSR6Z5D1J/matigIAAACYtdpfIflVVR2d5JOZLh35YmvNJSQAAADAulhVgFFVd0ry0iT/k6SS7FdVj2yt/cdaFgcAAACQrDLASPLcJLdqrZ2ZJFV1jSTvTiLAAAAAANbcau+B8eOF8KL7UpIfr0E9AAAAABeyYguMqrpH7zyxqt6T5E2Z7oFx7ySfXuPaAAAAAJJs+RKSu8x0n53klr17Y5Kd1qQiAAAAgEVWDDBaaw9dr0IAAAAAlrPaXyHZL8ljkuw7O01r7a5rUxYAAADAJqv9FZJ3JHl5kn9P8qs1qwYAAABgCasNMH7WWnvhmlYCAAAAsIzVBhgvqKqnJnl/kp8v9GytfWZNqgIAAACYsdoA4zpJHpzk1tl0CUnrjwEAAADW1GoDjHsn+c3W2i/WshgAAACApVxqleOdlmT3NawDAAAAYFmrbYGxe5IvVNWns/k9MPyMKgAAALDmVhtgPHVNqwAAAABYwaoCjNbaCWtdCAAAAMByVhVgVNWPM/3qSJJcJsmlk/yktbbrWhUGAAAAsGC1LTCusNBdVZXkbkluslZFAQAAAMxa7a+QXKBN3pHkdtu+HAAAAIALW+0lJPeYeXipJAcl+dmaVAQAAACwyGp/heQuM93nJflKpstIAAAAANbcau+B8dC1LgQAAABgOSsGGFX1tysMbq21p2/jegAAAAAuZEstMH6yRL/LJ3l4kismEWAAkCS52YtuNu8SGNRHH/PReZcAAFwCrBhgtNaeu9BdVVdI8rgkD03yb0meu9x0AAAAANvSFu+BUVV7Jnl8kgcmOTrJDVprP1jrwgAAAAAWbOkeGM9Oco8kRya5Tmvt3HWpCgAAAGDGpbYw/AlJrpzkb5J8s6p+1P9+XFU/WvvyAAAAALZ8D4wtBRwAAAAAa05AAQAAAAxPgAEAAAAMT4ABAAAADE+AAQAAAAxvLgFGVe1eVW+pqi9U1RlVddOq2rOqjq2q/+7/95hHbQAAAMB45tUC4wVJ3ttau1aS6yU5I8mTkxzXWts/yXH9MQAAAMD6BxhVtVuSWyR5eZK01n7RWjsnyd2SHN1HOzrJ3de7NgAAAGBM82iBsV+SjUleWVWfraqjqurySfZurX2rj/PtJHsvNXFVHVZVJ1bViRs3blynkgEAAIB5mkeAsWOSGyT5l9ba9ZP8JIsuF2mttSRtqYlba0e21g5qrR20YcOGNS8WAAAAmL95BBhnJTmrtfbJ/vgtmQKNs6vqSknS/39nDrUBAAAAA1r3AKO19u0kX6+q3+69bpPk80mOSXJI73dIkneud20AAADAmHac03Ifk+R1VXWZJF9K8tBMYcqbqurhSb6a5D5zqg0AAAAYzFwCjNbayUkOWmLQbda5FAAAAGA7MI97YAAAAABsFQEGAAAAMDwBBgAAADA8AQYAAAAwPAEGAAAAMDwBBgAAADA8AQYAAAAwPAEGAAAAMDwBBgAAADA8AQYAAAAwPAEGAAAAMDwBBgAAADA8AQYAAAAwPAEGAAAAMDwBBgAAADA8AQYAAAAwvB3nXQAAwHo44Ra3nHcJDOyWHzph3iUAsAVaYAAAAADDE2AAAAAAwxNgAAAAAMMTYAAAAADDE2AAAAAAwxNgAAAAAMMTYAAAAADDE2AAAAAAwxNgAAAAAMMTYAAAAADDE2AAAAAAwxNgAAAAAMMTYAAAAADDE2AAAAAAwxNgAAAAAMMTYAAAAADDE2AAAAAAw9tx3gUAAACTFz/h3+ddAgN79HPvMu8SkiTPeNC95l0CA3vKa9+yZvPWAgMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABje3AKMqtqhqj5bVe/qj/erqk9W1ZlV9caqusy8agMAAADGMs8WGI9LcsbM42cleV5r7ZpJfpDk4XOpCgAAABjOXAKMqrpKkjslOao/riS3TvKWPsrRSe4+j9oAAACA8cyrBcbzkzwpya/64ysmOae1dl5/fFaSfZaasKoOq6oTq+rEjRs3rnmhAAAAwPyte4BRVXdO8p3W2kkXZfrW2pGttYNaawdt2LBhG1cHAAAAjGjHOSzzZknuWlV3THK5JLsmeUGS3atqx94K4ypJvjGH2gAAAIABrXsLjNbaX7XWrtJa2zfJ/ZL8Z2vtgUk+mORefbRDkrxzvWsDAAAAxjTPXyFZ7C+TPL6qzsx0T4yXz7keAAAAYBDzuITkAq2145Mc37u/lOTG86wHAAAAGNNILTAAAAAAliTAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIYnwAAAAACGJ8AAAAAAhifAAAAAAIa37gFGVV21qj5YVZ+vqtOr6nG9/55VdWxV/Xf/v8d61wYAAACMaR4tMM5L8oTW2gFJbpLkUVV1QJInJzmutbZ/kuP6YwAAAID1DzBaa99qrX2md/84yRlJ9klytyRH99GOTnL39a4NAAAAGNNc74FRVfsmuX6STybZu7X2rT7o20n2nlddAAAAwFjmFmBU1S5J3prk/7bWfjQ7rLXWkrRlpjusqk6sqhM3bty4DpUCAAAA8zaXAKOqLp0pvHhda+1tvffZVXWlPvxKSb6z1LSttSNbawe11g7asGHD+hQMAAAAzNU8foWkkrw8yRmttX+aGXRMkkN69yFJ3rnetQEAAABj2nEOy7xZkgcnObWqTu79/jrJEUneVFUPT/LVJPeZQ20AAADAgNY9wGitfSRJLTP4NutZCwAAALB9mOuvkAAAAACshgADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABieAAMAAAAYngADAAAAGJ4AAwAAABjecAFGVd2+qr5YVWdW1ZPnXQ8AAAAwf0MFGFW1Q5J/TnKHJAckuX9VHTDfqgAAAIB5GyrASHLjJGe21r7UWvtFkn9Lcrc51wQAAADMWbXW5l3DBarqXklu31r7k/74wUl+r7X26JlxDktyWH/420m+uO6FXrLtleS78y4CtsB2yvbAdsrobKNsD2ynbA9sp9ve1VtrGxb33HEelVwcrbUjkxw57zouqarqxNbaQfOuA1ZiO2V7YDtldLZRtge2U7YHttP1M9olJN9IctWZx1fp/QAAAIBfY6MFGJ9Osn9V7VdVl0lyvyTHzLkmAAAAYM6GuoSktXZeVT06yfuS7JDkFa210+dc1q8bl+ewPbCdsj2wnTI62yjbA9sp2wPb6ToZ6iaeAAAAAEsZ7RISAAAAgAsRYAAAAADD264CjKq6YlWd3P++XVXf6N3nVNXnL8Z8D62qjTPzPrmqDtiWtS+z3K9U1V5rOP99q+q03n1QVb1wG8332VV1elU9e1H/vavqXVV1SlV9vqres4X5PK2q/nBb1LQ1qurwmW3n5Ko6oqr+T1U9ZIVp9q2qB8w83mbrc1tYh/fGZ6vqv6vqfVX1+zPD1+U17HVceebxURf1PVpVV66qt/TuA6vqjhdhHrevqk9V1Rf6en5jVV1tC9Msu82s9b5grdgnb/X813ufvHg9vvoizHv3qvqzbVEn205Vnd9f09P7MfcJVbWm53Sz2y/zM/PaL/w9eR2WeXhVPXGtl7Nomef2/xccs7di2oV1dEpVfWb2vGWJcT/W/292nrctVdVd1+N12t5UVauq5848fmJVHd67Vzwv38J81+y1XLScxZ8nTu7HzC0e31fany4+511lLRf7eL89GeomnlvSWvtekgOTaaNJcm5r7TlVtW+Sd13M2b+xtfboizmPYbXWTkxy4jaa3WFJ9mytnb+o/9OSHNtae0GSVNV1t1DT326jei6K57XWnrMV4++b5AFJXp9s8/V5sa3Xe6OqbpXkbVV1q9baGev4Gh6a5LQk30yS1tqfXNQZtda+meRe/eGBSQ5KsmLYNquqrp3kRUnu2lo7o/e7a6Zt5GsrLHeobWZbsE++6NZpn5xc/PW4e5I/S/KSizEPtr2fttYOTJKq+o1Mx6Zdkzx1dqSq2rG1dt76l8cauuC1/3Ww6Ji9WrPvj9sleWaSW86OsPDeaK0thBv7ZuY8b1tqrR0Tv6q4lJ8nuUdVPbO19t3ZAa21l16M+e6bNXotl7DU54mLe3w/NDPnvFthm543jXz82K5aYGzBDlX1r/3biPdX1U5JUlXXqKr3VtVJVfXhqrrWamdYVQdX1fFV9Zb+Tevrqqr6sBtV1cd6uvupqrpCVX2oqg6cmf4jVXW9qtqlql5ZVadW1eeq6p5LLOvxVXVa//u/vd++M8s9o9excx92w6o6oT+v91XVlWb6n1JVpyR51KLn8q7efeOq+nhN36p/rKp+e4l6qqZv9U7rdd+39z8myS5JTlroN+NKSc5aeNBa+9zM/P6yz+eUqjqi93tVVd1rC8/n+Kp6Vl/H/1VVf9D771BVz+n1fa6qHrPSfFbxWl/w7UJVXbOqPlCbkvtrJDkiyR/0VPPPF63PPavqHb2OT1QPbvo8X9Gfw5eq6rGrqWUNbLP3Rmvtg5nusnxYn8fsa3hETS1vPldVz+n99q6qty9sk9W/BVlhez9jca19/gcleV1f/zv1dXpQn+7cqnpGn/8nqmrvxbUtjDeznNNq+qnmpyW5b5/vfWtqZbKhj3epqjpz4fGMv0zyDwvhRV8vx7TWPtSnm61tr6r6Su+e3Wau2J/f6VV1VJKaqfNBfXs/uapeVlU7bOl1GZR98qb+p9R89slLrcO7VNUn+7I+MPN+WW5/dUSSa/Tt8dl93R1X077x1Kq6W5/+8lX17v5cT+vvp1tX1Ttmlv1HVfX2LdXI1mmtfSfTPvnRfTs5tKqOqar/THLccq9ZklTVQ/p74JSqek3vt+S+c1Z/L3y4z3PFb7hZHzW1IPu7mdf5Wr3/hfZ3VfWwqnr+zLSPqKrn9e4LbROLlnNgTcfaz9V0fN+j9z++ql7Q9xWnVdWNe/+F/dvJNe2b79T3kSfXtI9eqHO/Pt6pVfX3M8u74Jvqi7jd7ZrkB336g/v0xyT5fO+3sH0vPs9bclm18nFoudfg0Kp6ce9e7rzoHTUdP06vqsNW85pfApyX6ZzyzxcPqM3Py1f6LPDsqvp03x4f2Sdf/FouN16q6i9m+v9d77fk+ehqn1RtfnzfUFXH9vkcVVVfrU0tPS90nlQXPue9U12M42gtfy5z2sw4sy1fjq+q51fViUket9rlrLvW2nb5l+TwJE/s3ftmehMc2B+/KcmDevdxSfbv3b+X5D+XmNehSTYmOXnmb6ckByf5YZKrZAp7Pp7k5kkuk+RLSW7Up981U2uWQ5I8v/f7rSQn9u5nLfTvj/fo/7+SZK8kN0xyapLLZzoRPT3J9fvzaklu1sd/RZInJrl0ko8l2dD73zfTT84myeeS3KJ3PzvJab374CTvmq23d/9hkrcusU7umeTYTD9nu3emb5av1Iedu8xrcrsk5yT5YJKnJLly73+HXu/O/fGe/f+rMqXqKz2f45M8t3ffMckHevefJnnLzPPYc6X5LLHtfGPmtb5dNt+ePpnkj3v35ZLsPLv+llifL0ry1N596yQnzyznY0ku21/n7yW59Hb43njxon53T/Ifi17DKyb5YnLBLxvt3v+/Mcn/7d07JNktK2/vy9V6fJKDZmq44HGm98hdevc/Jvmb2dpmpjl3Zp2cttTzy/Tt5UK9t83S743PJLneCut/tra9knxliW3mhUn+tnffqT+HvZL8TpJ/X9hOMn3r/ZC13mYG3e7skzdfJxdln7x4PT40yR7Z9D79k2zavx6eJfZXmXm/9PF2TLLrzPZ9ZqYA7p5J/nVmvN16/y/MrJfXp79X/V3s99uFXvNMx9+9++t+VjYda5d7zX43yX8l2asP2+zYvHhZ2XzfuXOSy/Xu/dPfW/7W5bU/P5vvG+/b+38lyWN6958lOap3X2h/l2m/9j/ZdKz5WJLrrLBNHJ5N+/fPJbll735aNu1jj1/YByS5xcy2Mrt/OzDJL5Mc0B/fKP0YkKmFwkN696MuznY3s46+kOmYccPe/+AkP0my3xLb98HZ/DxvyWVlmePQFl6DQ9PPNbLEedGidb1Tpm/frzjvbW0dtuVz+/bxlUzHjCcmOXyJbe74LP1Z4LBsOue7bKZWD/st8VouN95tMwUo1V/Ld/Vtd98scw6zqP7Ds/nniQ8u3paSvDjJX/Xu22fT+d6yy8jm55GrOo5m6eP9Sucys8f12fV+fJKXzHvb2NLfdnUJyRZ8ubV2cu8+Kcm+VbVLkt9P8uYejibThruUCzW76dN8qrV2Vn98cqYX/YdJvtVa+3SStNZ+1Ie/Ocn/q6q/SPKwTCcByXRCer+F+bbWfrBo2TdP8vbW2k/6fN6W5A8y7cy/3lr7aB/vtUkem+S9Sa6d5Nhe4w5JvlVVu2f64PihPv5rMoUHi+2W5Oiq2j/TG+nSS4xz8yRvaFOT5LOr6oRMB5plm8C11t5XVb+Z6Q16hySfram5/R8meWVr7X/7eN9fNOlvL/V8Zoa/rf8/KdP6T5/nS1tv2tRa+35f1krzmfW8NtPkq6pu2v9fIck+rbW39/n+rPdf7mkn07q6Zx//P2v6dn3XPuzdrbWfJ/l5VX0n08nlWcvMZ61c3PfGYkutjB8m+VmSl/fUeeHygVsneUiS9G3ph1W10vZ+oVpXUc8vZpZ3UpI/WuXzWMorkrwzyfMzvYdfudLIVXXFTB/Id05yZFv9ZUm3SHKPJGmtvbuqFvYJt8l0wPl0f112SvKdrXsKw7BPnvM+udtsPVbVdZK8saYWIpdJ8uWZcZfaXy1WSf6hqm6R5FdJ9unjnZrkuVX1rEwnbh/uy3tNkgdV1SuT3DR9f8CaO3bmWLvca3brJG9uven2EsfmlVw6yYtrauF0fqaAkPWx0iUks+dL9+jdS+7vamqhc+eqOiNTkHFqTa1Zl90mqmq3TPu0E3qvo5O8eWaUN/TpPlRVu/Z94C5JXllTy7LzMu3jXr/EMeBm6edSmfaVz1ri+a12u5u9hOSmSV7dzxGT6Rjy5WWmW+2yljoOfaQPW+o1mHWh86Le/7FV9ce9+6qZQpPvraLO7Vpr7Uc13a/hsUl+usKoS30WuG2S69amFmO7ZVpvv1g07XLj3bb/fbb336X3/1pWfz662eeJJdw8yR8nSWvtvTPne1nNMlprbSuOo4uP94/L8ucyK3njFobP3SUpwPj5TPf5mU78L5XknBV29Bdlvsuus9ba/1bVsUnuluQ+mT6IXFxticeV5PTW2k1nB/QDxWo8PVNK+Mc1Xat+/MWscVNx08Hu9ZkOTgtJ5pYs+XxmLLwGK67/VcxnHla9/axjDRf3vXH9JGfM9mitnVdTc9HbZGqR8ehMB+ltUeuW/LL12Dibr+Pz0i+Tq+nmdpfZ0oxaa1+vqrOr6tZJbpzkgUuMdnqSGyQ5pfV7QNTUzHGXxcvN1IJna1SSo1trf7WV043IPnmAffISXpTkn1prx1TVwZm+QVqwmnX7wCQbMn2j+cuaLpG6XGvtv6rqBpm+Hfv7qjqutfa0TCHgv2cKON/cBr2ednvXvzw4P5sCz5/MDF7yNVthdqvZd/55krOTXK+P+7OLUz/bzGrPl5LkqCR/nenb3RXD+q2w1P7x6UlOaK3ds39J9M0VjgGLp19sq7e71trHe5P9hctBf7LS+Ktc1kr7yq15DZJMlxxkCppu2o9bx2frzx+2Z8/P1Lp1pe1wqfVamVq8vG92xL4+N+u1zHi3S/LM1trLFvXfNxftfHRrrXYZFzqOVtWjkjyiD9/am9HPnqcmF97WVvsemZtL0j0wLqR/C/flqrp3csE1xNfbBrP+YpIrVdWN+nyvUFULb6ajMjUP//TMt3rHZvNrn/dYNL8PJ7l7Ve1cVZfPlNR9uA+72kLrgEw3pPlIX/6GmVYDl66q322tnZPknP4Nd7L0h69kSh6/0bsPXWacD2e6N8AONd0D4BZJPrXMuAvP69a16XrwKyS5RqYU89gkD50ZtueiSZd8Pistq8/zkQvrvc/zosxnM621Hyc5q6ru3udx2V73j5NcYZnJPpy+rvtO87sL3wCP6qK+N6rqlpma4v3rov67ZGoG+Z5MB/2FeR2X6XKfhWsVd8vK2/tyVlr/y/lKNn1gvWuW/lZ7qfkelemb9Te3pW+K+I9JnlJVvzPTb+dllrvcjcc+lOn9nKq6Q6Ymvcm0vu5V0035UtP9Va6+zDy2O/bJ67tPXsWyDlnF+IvfI7sl+U7/IHyrJFdPkprumP6/rbXXZrpU5gbJBTfg+2aSv8m2+5DEjL49vDRTE/WlPgAu+Zol+c8k966pJdnssfkr2fK+c7dMrZ5+leTBmVodMaYl93ettU9m+qb/AektJ7L8NpE+zQ+T/KD6PQgyvfYnzIyycG+emyf5YR9/92xqDfvwJL+oqof18S5VVdfvwz6aTS1FVtpXbtV2V9N9KHbIllszLLWvW4ttfKnzot2S/KCHF9dKcpNttKztQv/y802Zto+t8b4kf1pVl06Sqvqtfsxe/FouN977kjysn8OmqvZZOP/ahj6a6QuUVNVts+l8byWb1b/UcbS19s+ttQP733I3+1zuXObsJL9RU4vxyya580V7avNziQ4wugcmeXhNN1A7PdM3cUtZuJHfwt+yNwZqrf0i0076RX2+x6anV621k5L8KJufqP19kj1quoHKKUlutWh+n8nUtPlTme6/cFRrbaE50xeTPKqmJn57JPmXvvx7JXlWn9/JmZplJ9M1T/9cU5O25a57+Mckz6yqz2b5dPjtma5zPCXTAe1JrbVvL7dOuhsmObGqPpfpmsCjWmufbq29N1NzpRN7XZv9FNcWns9yjsoUjnyuT/OAizifpTw4U1O+z2W6LvT/y7Quzq/phkuLbzZ0eJIb9vGPyOo+FIxga98b/5Xp25p7tpkbWHZXSPKuvg4+kuTxvf/jktyqqk7N1DzugC1s78t5VZKX9jpWm4L/a5Jb9ud30yydKH8wyQF9vgs3QFy4KeKSH7Zaa6f25/XqqvpiVX00070rFu52/ZxMB8rPZrrOcSl/l+QWVXV6piamX+vz/nymA9T7+7o8NtPNcS9J7JMvbK32yUs5PNMlPCcl+e4Wxk1vZfTRvq6eneR1SQ7q7+mHZPr2Npmun/9Uf55PzbSOF7wu06U3i/cbXHQ79ffF6Uk+kOT9mfYrS1nyNWutnZ7kGUlO6NvtP/XxV7PvfEmSQ/o411pmHNbGTov2jUdsYfyV9ndvSvLRhXB3hW1i1iFJnt2PUQdmug/Ggp/1/dhLs+nD6BFJDu/j75/pC4771PRT26cluUsf73GZ9q2nZrrMaSmr3e4uWEeZmsMfsswXErMWn+et1TZ+ofOiTJch7tiPK0ck+cQ2Wtb25LlZ/pxpOUdluhnrZ2q6KeXLMh1DF7+WS47XWnt/pnO3j/fX4y3Z+i/L/nzR+3HfRcP/Lslt+3LvneTbmQKKlbwqFz7n3erj6HLnMq21X2Z6334q0/nSF5adyaAWbuTFNlLTt1DHJ7lWT20vzrz2zXQt8bW3NC6wbdT0CyLPa639wRZHZnj2yfNX0933P9tae/m8awE2qelS3+e11o7bBvM6PtNNFy9RPxcOF0dv4XB+v+zjppm+9DjwIszHcXTGr0MLjHVTVQ/JlHA95eKeKAPrr6qenOStSS4J96D4tWefPH+9pcd1M12WBQygqnbvrSp/ui3CC2BZV8t0Y/ZTMl3O+ogtjH8hjqMXpgUGAAAAMDwtMAAAAIDhCTAAAACA4QkwAAAAgOEJMACAuaiq8/tPxZ1SVZ9Z6edytzCfg/svKgAAl2DL/d48AMBa++nCT8pV1e2SPDPJLedaEQAwLC0wAIAR7JrkB0lSk2dX1WlVdWpV3Xel/rOq6kZV9dmqusY61w8ArDEtMACAedmpqk5OcrkkV0py697/HkkOTHK9JHsl+XRVfSjJ7y/TP0nSL0F5UZK7tda+tj5PAQBYLwIMAGBeZi8huWmSV1fVtZPcPMkbWmvnJzm7qk5IcqMV+v8oye8kOTLJbVtr31z/pwIArDWXkAAAc9da+3imVhUbLuIsvpXkZ0muv82KAgCGIsAAAOauqq6VZIck30vy4ST3raodqmpDklsk+dQK/ZPknCR3SvLMqjp4fasHANaDS0gAgHlZuAdGklSSQ1pr51fV25PcNMkpSVqSJ7XWvr1C/2slSWvt7Kq6c5L/qKqHtdY+ud5PCABYO9Vam3cNAAAAACtyCQkAAAAwPAEGAAAAMDwBBgAAADA8AQYAAAAwPAEGAAAAMDwBBgAAADA8AQYAAAAwvP8fMJlrY2CeYNwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1080x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the most repeated books\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # for Visualization\n",
    "\n",
    "common_book = pd.DataFrame(list(wikilink_book_counts.items()), columns = [\"Book\", \"Number\"])\n",
    "\n",
    "plt.figure(figsize = (15,9))\n",
    "sns.barplot(x = \"Book\", y = \"Number\", data= common_book[:6])\n",
    "plt.tight_layout()\n",
    "plt.title(\"Most Reapeated book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715bdbde",
   "metadata": {},
   "source": [
    "#### Wikilinks to Index and vise versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40b5663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_index = {link: i for i, link in enumerate(links)}\n",
    "index_link = {i: link for link, i in link_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f20c82a",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e84cd4",
   "metadata": {},
   "source": [
    "#### Basically, we want to create a pair of tuples with each book and its respective wikilinks. This data will be used as a training set in our supervised learning algorithm to find the optimal embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a4691f",
   "metadata": {},
   "source": [
    "#### Use Generator to see if the function prints a tules of index, as its a large data set and will take some time to load, generate it and if it works, then you can create a list comprehension for it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e697330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a function to generate next item, as it does not save in the memory until you call next\n",
    "## yield to call generator and next to see the next result\n",
    "\n",
    "def create_pairs(books,links):\n",
    "    \n",
    "    for book in books:\n",
    "        for link in book[2]:\n",
    "            if link.lower() in links:\n",
    "                yield((book_index[book[0]], link_index[link.lower()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "870dd4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 265)\n",
      "(0, 353)\n",
      "(0, 265)\n",
      "(0, 4515)\n",
      "(0, 353)\n",
      "(0, 15961)\n",
      "(0, 9542)\n",
      "(0, 1028)\n",
      "(0, 25616)\n",
      "(0, 661)\n"
     ]
    }
   ],
   "source": [
    "## CHecking if the values are correct\n",
    "yolo = create_pairs(books,links)\n",
    "\n",
    "for i in range(10):\n",
    "    print(next(yolo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc44bb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "\n",
    "for book in books:\n",
    "   \n",
    "    pairs.extend((book_index[book[0]], link_index[link.lower()]) for link in book[2] if link.lower() in links)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "370f6ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs: 772798, links: 41758, books: 37020\n"
     ]
    }
   ],
   "source": [
    "print(f\"pairs: {len(pairs)}, links: {len(links)}, books: {len(books)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8523d1a1",
   "metadata": {},
   "source": [
    "#### S0, We have 772798 pairs of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f570ec8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Lost in language  &  sound', 'misogynistic')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Some Examples\n",
    "\n",
    "index_book[pairs[501][0]], index_link[pairs[501][1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a11f3f8",
   "metadata": {},
   "source": [
    "#### TRAINING SET : Since we are not trying to build a model to predict, we do not need to split our data between training and validation. We want to get the most optimal embedding, so its a better idea to use all the data available for training for better accuracy. \n",
    "\n",
    "#### Embedding is just a way to vectorize the words. Just like one hot encoding, we use embedding to get getter vector representation. unlike one hot encoding, embedding is also usefull as it groups similar categories together. We can then take cosine distance (dot product) to get the similarity between two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3dc2953",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_set = set(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd776871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, we basically want to create a batch generator that gives the training example in array(batchsize,3)\n",
    "# which is book_index, link_index and the positive/negative response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be45a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(pairs, n_positive = 50, negative_ratio = 1.0):\n",
    "    \n",
    "    batch_size = n_positive * (1 + negative_ratio)\n",
    "    batch = np.zeros((batch_size, 3))\n",
    "    neg_cl_lab = 0   ## Since we will be using classification, the negative examples will be 0\n",
    "    \n",
    "    # Create a Generator.. Basically to be efficient with the memory\n",
    "    while True:\n",
    "        # Creates an array from a randomly chosen sample set\n",
    "        for i, (book_id, link_id) in enumerate(random.sample(pairs, n_positive)):\n",
    "            batch[i, :] = (book_id, link_id, 1)\n",
    "\n",
    "        # Increment i by 1 at each loop\n",
    "        i =i + 1\n",
    "        \n",
    "        # Continue to create the batch with negative examples (wikilinks not in book) until the batch size reached\n",
    "        while i < batch_size:\n",
    "            \n",
    "            # random selection of book and link\n",
    "            r_book = random.randrange(len(books))\n",
    "            r_link = random.randrange(len(links))\n",
    "            \n",
    "            # Check if the combination exists in the positive pair\n",
    "            if (r_book, r_link) not in pairs_set:\n",
    "                \n",
    "                # Add to batch and increment index\n",
    "                batch[i, :] = (r_book, r_link, neg_cl_lab)\n",
    "                i =i + 1\n",
    "                \n",
    "        # shuffle order\n",
    "        np.random.shuffle(batch)\n",
    "        yield {'book': batch[:, 0], 'link': batch[:, 1]}, batch[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c5a3de",
   "metadata": {},
   "source": [
    "#### Lets see some examples of creating a batch of index and then calling the book, link title out of that index position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d3d841b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book: Like a Conquered Province      link: medieval                       label: 0.0\n",
      "Book: The Lion (2010 novel)          link: impeachment in the united states label: 0.0\n",
      "Book: The Princess of Dhagabad       link: herodias                       label: 1.0\n",
      "Book: Swords Against Darkness        link: category:oxford university press reference books label: 0.0\n",
      "Book: Between the Bridge and the River link: category:2006 novels           label: 1.0\n",
      "Book: The Pit (Penswick novel)       link: category:swedish books         label: 0.0\n"
     ]
    }
   ],
   "source": [
    "combo,y = next(generate_batch(pairs, n_positive=2, negative_ratio=2)) # since it returns a tuple, we assign two variables\n",
    "\n",
    "for i_book,i_link,i_y in zip(combo[\"book\"],combo[\"link\"],y):\n",
    "    print(f\"Book: {index_book[i_book]:30} link: {index_link[i_link]:30} label: {i_y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9369a6ec",
   "metadata": {},
   "source": [
    "#### The above function takes in pairs of each book with its respective links in tuple type. takes in the number of positive sample you want to generate at a time and also the number of negative result ratio and outputs a array of (book,link,positive/negative) batch thats randomly chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30219ca9",
   "metadata": {},
   "source": [
    "## Steps till now\n",
    "\n",
    "#### Firstly, We loaded the scraped wikipedia data in JSON format and created a list of all the book info.\n",
    "\n",
    "#### We then wanted to create a training dataset, which led us to create a book title list and corresponding links list.\n",
    "\n",
    "#### We then did some cleaning and analysis in order to get better training data for our ML model\n",
    "\n",
    "#### We then created index for each books and its links, inorder to vectorize it for the ML model\n",
    "\n",
    "#### We then created a pair of tuples for the indices of books,links\n",
    "\n",
    "#### We then created a function that generates a batch in the array form, perfect input and output value type for our ML model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eae20d",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK EMBEDDING MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c6b3f7",
   "metadata": {},
   "source": [
    "#### Neural Network Model Using Keras\n",
    "\n",
    "- When we create a model, it is just the dim,shape and what to use\n",
    "- layer is just adding the layers, can be Dense layer, Embedding layer, LSTM layer\n",
    "- activation is adding what function to use to convert the inputs\n",
    "- compile is used to see how to optimize and the loss func to use to back propagate\n",
    "- fit is now finally inputs going in the model to find optimal parameters, needs epochs, batch_size etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cbea22",
   "metadata": {},
   "source": [
    "### \n",
    "- Usually for embedding, we need: max_vocab_length, max_input_length\n",
    "- Output shape is 3D tensor (batch_size, input_length, output_dim)\n",
    "- For more info on embedding, check out the keras- embedding documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a102ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "book (InputLayer)               [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "link (InputLayer)               [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "book_embedding (Embedding)      (None, 1, 40)        1480800     book[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "link_embedding (Embedding)      (None, 1, 40)        1670320     link[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dot_product (Dot)               (None, 1, 1)         0           book_embedding[0][0]             \n",
      "                                                                 link_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 1)            0           dot_product[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            2           reshape[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,151,122\n",
      "Trainable params: 3,151,122\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 14:13:47.132516: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-20 14:13:47.133658: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "def embedding_model(embedding_size = 60):\n",
    "    \n",
    "    # Since we will be using functional API to connect the layers in the neural network\n",
    "    # We create input placeholders of 1D\n",
    "    book = Input(name = 'book', shape = [1])\n",
    "    link = Input(name = 'link', shape = [1])\n",
    "    \n",
    "    # book embeddings: It creates 3D output of (book title, integer rep, vector rep)\n",
    "    # by taking in 2D input (book title, integer rep)\n",
    "    # output_dim is our choice, basically the number of vec dim rep\n",
    "    \n",
    "    # since input_length is 1, we do not need to specify it.\n",
    "    book_embedding = Embedding(name = 'book_embedding',\n",
    "                               input_dim = len(book_index),\n",
    "                               output_dim = embedding_size)(book) # the (book) is the input value\n",
    "    \n",
    "    # Embedding the link of 3D shape\n",
    "    link_embedding = Embedding(name = 'link_embedding',\n",
    "                               input_dim = len(link_index),\n",
    "                               output_dim = embedding_size)(link)\n",
    "    \n",
    "    # Dot product the vectors. Output gives 3D, (batch,1,1)\n",
    "    dot_output = Dot(name = 'dot_product', normalize = True, axes = 2)([book_embedding, link_embedding])\n",
    "    \n",
    "    # outputs single number to use loss function\n",
    "    dot_output = Reshape(target_shape = [1])(dot_output)\n",
    "    \n",
    "    # Since we are doing a classification task, we use sigmoid function to convert the vals between 0-1\n",
    "    dot_output = Dense(1, activation = 'sigmoid')(dot_output)\n",
    "    \n",
    "    # Create a neural network model that takes inputs and outputs \n",
    "    model = Model(inputs = [book, link], outputs = dot_output)\n",
    "    \n",
    "    # we use adam optimizer since it is quicker and binary crossentropy as it is binary task (0,1)\n",
    "    model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate the model \n",
    "model = embedding_model(embedding_size = 40)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af284766",
   "metadata": {},
   "source": [
    "#### There are nearly 4.0 million weights (parameters) that need to be learned by the neural network. Each of these represents one number in an embedding for one entity. During training, the neural network adjusts these parameters in order to minimize the loss function on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2234d95e",
   "metadata": {},
   "source": [
    "# TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a5bcfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "671/671 - 88s - loss: 0.6629 - accuracy: 0.6244\n",
      "Epoch 2/12\n",
      "671/671 - 94s - loss: 0.6390 - accuracy: 0.6667\n",
      "Epoch 3/12\n",
      "671/671 - 95s - loss: 0.6368 - accuracy: 0.6667\n",
      "Epoch 4/12\n",
      "671/671 - 96s - loss: 0.6365 - accuracy: 0.6667\n",
      "Epoch 5/12\n",
      "671/671 - 96s - loss: 0.6365 - accuracy: 0.6667\n",
      "Epoch 6/12\n",
      "671/671 - 96s - loss: 0.6365 - accuracy: 0.6667\n",
      "Epoch 7/12\n",
      "671/671 - 98s - loss: 0.6365 - accuracy: 0.6667\n",
      "Epoch 8/12\n",
      "671/671 - 97s - loss: 0.6365 - accuracy: 0.6667\n",
      "Epoch 9/12\n",
      "671/671 - 98s - loss: 0.6365 - accuracy: 0.6667\n",
      "Epoch 10/12\n",
      "671/671 - 99s - loss: 0.6365 - accuracy: 0.6667\n",
      "Epoch 11/12\n",
      "671/671 - 98s - loss: 0.6365 - accuracy: 0.6667\n",
      "Epoch 12/12\n",
      "671/671 - 99s - loss: 0.6365 - accuracy: 0.6667\n"
     ]
    }
   ],
   "source": [
    "n_positive = 1150 # number of samples in a batch\n",
    "\n",
    "generate = generate_batch(pairs, n_positive, negative_ratio = 2)\n",
    "\n",
    "# Train\n",
    "recommend = model.fit_generator(generate, epochs = 12, \n",
    "                        steps_per_epoch = (len(pairs) // n_positive),\n",
    "                        verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25a5f1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('book_recommed_wiki_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ae8cf5",
   "metadata": {},
   "source": [
    "#### Usually we predict new data using our model, but in this project, we just want to use the embedding parameters that we got to find similar books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "957fb6e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37020, 40)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Extracting the trained embeddings representing each book\n",
    "\n",
    "b_layer = model.get_layer(\"book_embedding\") # assigns the layer \n",
    "b_weights = b_layer.get_weights()[0]\n",
    "b_weights.shape # shape = (book title, vec rep of book title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfba2957",
   "metadata": {},
   "source": [
    "#### The shape above shows that, each book is represented by a 60 dimensional vector. \n",
    "\n",
    "#### We now need to normalize the 60d vector. normalizing is the process of finding a unit vector ( with magnitude 1) in the same director as the initial vector that we normalized. This step normalizes all the 37020 vectors, where each vector represents a book. This is done as the dot product of the query book vector and all the other vectors will give the cosine similarity which is used to recommend a book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ced44d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.22592951, -0.0582108 ,  0.3083123 , -0.11840917, -0.12364815,\n",
       "        0.15228452,  0.0852596 , -0.15666091,  0.15261719, -0.0037436 ,\n",
       "        0.28407413, -0.15441671,  0.09612592, -0.05674679,  0.01238544,\n",
       "       -0.01603526, -0.13152741, -0.12075199, -0.15884937,  0.09561033,\n",
       "        0.20556837,  0.13418259, -0.37168857, -0.02439806,  0.08285867,\n",
       "       -0.18842714,  0.12060835,  0.15655597, -0.30981612, -0.0611244 ,\n",
       "        0.17117448,  0.08981504,  0.05978707,  0.05466589, -0.11897089,\n",
       "        0.02789591, -0.01967595, -0.07671958,  0.33723456, -0.01420572],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_weights = b_weights / np.linalg.norm(b_weights, axis = 1).reshape((-1, 1)) # Normalizes all the vectors in the array\n",
    "b_weights[0] ## this array represents the first book in our list which is already normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28dadb04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.square(b_weights[0])) # you can see the magnitude of the vectors is 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef06039",
   "metadata": {},
   "source": [
    "# FINDING THE SIMILAR BOOKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2d87a3",
   "metadata": {},
   "source": [
    "#### After we take the dot product, the most similar books are represented with a 1 and the least are represented by -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0dc1ac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a function that takes the query book name, number of results as n, weights of all the books\n",
    "# that outputs the books similar or least similar with its correlation value\n",
    "\n",
    "def similar_books(name, weights, n =5, least_similar = False ):\n",
    "    \n",
    "    index = book_index\n",
    "    r_index = index_book\n",
    "    \n",
    "    \n",
    "    # tries to see if the query book name is amonst the books in our list, if not returns not found\n",
    "    \n",
    "    try:\n",
    "         co_sim = np.dot(weights,weights[index[name]]) # takes cosine similarities between vectors\n",
    "            \n",
    "    except KeyError:\n",
    "        print(f'{name} Not Found.')\n",
    "        return  \n",
    "    \n",
    "    sorted_idx = np.argsort(co_sim) # Sorts the correlation (-1 to 1) in ascending order \n",
    "    r_sorted_idx = sorted_idx[::-1] # Sorts the correlation (-1 to 1) in descending order \n",
    "    \n",
    "    if least_similar: # if True\n",
    "        \n",
    "        print(f\"The least similar books compared to {name} are:\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        for i in range(n):\n",
    "             print(f\"{r_index[sorted_idx[i]]:70} Similarity: {co_sim[sorted_idx[i]]}\")\n",
    "                  \n",
    "    else:\n",
    "        print(f\"The Most similar books compared to {name} are:\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        for i in range(n):\n",
    "             print(f\"{r_index[r_sorted_idx[i]]:70} Similarity: {co_sim[r_sorted_idx[i]]}\")         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85dfabf",
   "metadata": {},
   "source": [
    "# Examples of recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0af45c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Most similar books compared to Freud: His Life and His Mind are:\n",
      "\n",
      "\n",
      "Freud: His Life and His Mind                                           Similarity: 0.9999998807907104\n",
      "Star Trek Lives!                                                       Similarity: 0.5943354368209839\n",
      "Clouds Blur the Rainbow                                                Similarity: 0.5678695440292358\n",
      "Culture of Corruption                                                  Similarity: 0.5466879606246948\n",
      "Drummer Hoff                                                           Similarity: 0.5334361791610718\n"
     ]
    }
   ],
   "source": [
    "sim_book1 = similar_books(books[0][0],b_weights) # first book in out list of books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7d6a12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The least similar books compared to Freud: His Life and His Mind are:\n",
      "\n",
      "\n",
      "The Orchid Thief                                                       Similarity: -0.6840913891792297\n",
      "A Basket of Leaves                                                     Similarity: -0.5944517850875854\n",
      "Os Maias                                                               Similarity: -0.576063334941864\n",
      "The 120 Days of Sodom                                                  Similarity: -0.537731945514679\n",
      "Khalangama Hamala                                                      Similarity: -0.5291274189949036\n"
     ]
    }
   ],
   "source": [
    "sim_book2 = similar_books(books[0][0],b_weights,least_similar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5740c396",
   "metadata": {},
   "outputs": [],
   "source": [
    "book3 = books[2][0]  ## assigns the book title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4bbcbf42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Most similar books compared to My Real Children are:\n",
      "\n",
      "\n",
      "My Real Children                                                       Similarity: 1.0\n",
      "The Age of Louis XIV                                                   Similarity: 0.5981936454772949\n",
      "Four Upbuilding Discourses, 1844                                       Similarity: 0.5879479050636292\n",
      "Little People!                                                         Similarity: 0.5691173076629639\n",
      "The Black Obelisk                                                      Similarity: 0.5546696186065674\n"
     ]
    }
   ],
   "source": [
    "sim_book3 = similar_books(book3,b_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bada9753",
   "metadata": {},
   "source": [
    "#### You can see that the coefficient of similarity is 1 between itself. THAT MAKES SENSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde6e20b",
   "metadata": {},
   "source": [
    "### Highlight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f2439d",
   "metadata": {},
   "source": [
    "For a NLP project, the overall intuition involves:\n",
    "\n",
    "- tokenizing the words, removing STOPWORDS, stemming\n",
    "- Creating integer index, converting it to vector (use various libraries for this)\n",
    "- Embedding the words to get densed vector\n",
    "- Supervised learning to optimize the embedding parameters\n",
    "- Converting the vector back to words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a6d154",
   "metadata": {},
   "source": [
    "# CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3d5bfb",
   "metadata": {},
   "source": [
    "We created a pretty decent book recommendation system by following a simple concept that similar books will be referenced in the query book. We then created a neural network to develop a supervised learning to merge books and the referened books in order to get the embedding parameters, which was then used to find the cosine similarities.\n",
    "\n",
    "The embedding parameters found can be used in various ways to find the similar books. We could also find similar books based on the genre, authors etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d937f12",
   "metadata": {},
   "source": [
    "### How can we improve \n",
    "#### We can improve the model in various ways in order to get better embedding vectors:\n",
    "\n",
    "- We can clean the dataset further by removing common items in order to reduce the noise\n",
    "- We can optimize hyper-parameters in the neural network model to get better result\n",
    "- We can use a different info to do the supervised learning in order to get better model to recommend\n",
    "- We can try and use regression model instead of classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
